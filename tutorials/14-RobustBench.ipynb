{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('secml-malware': conda)",
   "metadata": {
    "interpreter": {
     "hash": "920e6f16d919fd37ee1019d40bfa14f341859d5274f6aed429526ec4c95ed979"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Testing attacks against RobustBench models\n",
    "\n",
    "In this tutorial, we will show how to correctly import [RobustBench](https://github.com/RobustBench/robustbench) models inside SecML, and how to craft adversarial evasion attacks against them using SecML.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Warning**\n",
    "\n",
    "Requires installation of the `pytorch` extra dependency.\n",
    "See [extra components](../index.rst#extra-components) for more information.\n",
    "\n",
    "</div>\n",
    "\n",
    "We start by installing the models offered by RobustBench, a repository of pre-trained adversarially robust models, written in PyTorch.\n",
    "All the models are trained on CIFAR-10.\n",
    "To install the library, just open a terminal and execute the following command:\n",
    "\n",
    "```bash\n",
    "pip install git+https://github.com/RobustBench/robustbench\n",
    "```\n",
    "\n",
    "After the installation, we can import the model we like among the one offered by the library ([click here](https://github.com/RobustBench/robustbench/tree/master/model_info) for the complete list):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from robustbench.utils import load_model\n",
    "from secml.utils import fm\n",
    "from secml import settings\n",
    "\n",
    "output_dir = fm.join(settings.SECML_DS_DIR, 'robustbench')\n",
    "model = load_model(model_name='Carmon2019Unlabeled', norm='Linf', model_dir=output_dir)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "This command will create a `models` directory inside the `secml-data` folder in your home directory, where it will download the desired model, specified by the `model_name` parameter.\n",
    "Since it is a PyTorch model, we can just load one sample from CIFAR-10 to test it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml.data.loader.c_dataloader_cifar import CDataLoaderCIFAR10\n",
    "from secml.ml.features.normalization import CNormalizerMinMax\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "dataset_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "train_ds, test_ds = CDataLoaderCIFAR10().load()\n",
    "normalizer = CNormalizerMinMax().fit(train_ds.X)\n",
    "pt = test_ds[0, :]\n",
    "x0, y0 = pt.X, pt.Y\n",
    "\n",
    "x0 = normalizer.transform(x0)\n",
    "input_shape = (3, 32, 32)\n",
    "\n",
    "x0_t = x0.tondarray().reshape(1, 3, 32, 32)\n",
    "\n",
    "y_pred = model(torch.Tensor(x0_t))\n",
    "\n",
    "print(\"Predicted classes: {0}\".format(dataset_labels[y_pred.argmax(axis=1).item()]))\n",
    "print(\"Real classes: {0}\".format(dataset_labels[y0.item()]))"
   ]
  },
  {
   "source": [
    "# Load RobustBench models inside SecML\n",
    "\n",
    "We can now import the pre-trained robust model inside SecML. Since these models are all coded in PyTorch, we just need to use the PyTorch wrapper of SecML.\n",
    "\n",
    "In order to do this, we need to express the `input_shape` of the data, and feed the classifier with the flatten version of the array (under the hood, the framework will reconstruct the original shape):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml.ml.classifiers.pytorch.c_classifier_pytorch import CClassifierPyTorch\n",
    "\n",
    "secml_model = CClassifierPyTorch(model, input_shape=(3,32,32), pretrained=True)\n",
    "y_pred = secml_model.predict(x0)\n",
    "print(\"Predicted class: {0}\".format(dataset_labels[y_pred.item()]))\n"
   ]
  },
  {
   "source": [
    "# Computing evasion attacks\n",
    "\n",
    "Now that we have imported the model inside SecML, we can compute attacks against it.\n",
    "We will use the iterative Projected Gradient Descent (PGD) attack, with `l2` perturbation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml.adv.attacks.evasion import CAttackEvasionPGD\n",
    "from secml.data import CDataset\n",
    "\n",
    "noise_type = 'l2'   # Type of perturbation 'l1' or 'l2'\n",
    "dmax = 0.5          # Maximum perturbation\n",
    "lb, ub = 0, 1       # Bounds of the attack space. Can be set to `None` for unbounded\n",
    "y_target = None     # None if `error-generic` or a class label for `error-specific`\n",
    "\n",
    "# Should be chosen depending on the optimization problem\n",
    "solver_params = {\n",
    "    'eta': 0.4, \n",
    "    'max_iter': 100, \n",
    "    'eps': 1e-3\n",
    "}\n",
    "\n",
    "pgd_ls_attack = CAttackEvasionPGD(\n",
    "    classifier=secml_model,\n",
    "\tdouble_init_ds=test_ds[0, :],\n",
    "    distance=noise_type, \n",
    "    dmax=dmax, \n",
    "    lb=lb, ub=ub,\n",
    "    solver_params=solver_params,\n",
    "    y_target=y_target)\n",
    "\n",
    "y_pred_pgd, _, adv_ds_pgd, _ = pgd_ls_attack.run(x0, y0)\n",
    "print(\"Real class: {0}\".format(dataset_labels[y0.item()]))\n",
    "print(\"Predicted class after the attack: {0}\".format(dataset_labels[y_pred_pgd.item()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml.figure import CFigure\n",
    "%matplotlib inline\n",
    "\n",
    "img_normal = x0.tondarray().reshape((3,32,32)).transpose(2,1,0)\n",
    "img_adv = adv_ds_pgd.X[0,:].tondarray().reshape((3,32,32)).transpose(2,1,0)\n",
    "\n",
    "diff_img = img_normal - img_adv\n",
    "diff_img -= diff_img.min()\n",
    "diff_img /= diff_img.max()\n",
    "\n",
    "fig = CFigure()\n",
    "fig.subplot(1,3,1)\n",
    "fig.sp.imshow(img_normal)\n",
    "fig.sp.title('{0}'.format(dataset_labels[y0.item()]))\n",
    "fig.sp.xticks([])\n",
    "fig.sp.yticks([])\n",
    "\n",
    "fig.subplot(1,3,2)\n",
    "fig.sp.imshow(img_adv)\n",
    "fig.sp.title('{0}'.format(dataset_labels[y_pred_pgd.item()]))\n",
    "fig.sp.xticks([])\n",
    "fig.sp.yticks([])\n",
    "\n",
    "\n",
    "fig.subplot(1,3,3)\n",
    "fig.sp.imshow(diff_img)\n",
    "fig.sp.title('Amplified perturbation')\n",
    "fig.sp.xticks([])\n",
    "fig.sp.yticks([])\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}